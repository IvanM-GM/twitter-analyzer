# План реалізації проекту

## 1. Етап 1: Базова архітектура та парсинг Twitter (2 тижні)

### 1.1 Налаштування проекту

#### Backend (Python + FastAPI)

```bash
# Створення структури проекту
mkdir twitter-analyzer-backend
cd twitter-analyzer-backend

# Створення віртуального середовища
python -m venv venv
source venv/bin/activate  # Linux
# або
venv\Scripts\activate  # Windows

# Встановлення залежностей
pip install fastapi uvicorn requests beautifulsoup4 selenium pillow opencv-python python-multipart
```

#### Структура файлів Backend:

```python
# requirements.txt
fastapi==0.104.1
uvicorn==0.24.0
requests==2.31.0
beautifulsoup4==4.12.2
selenium==4.15.2
pillow==10.1.0
opencv-python==4.8.1.78
python-multipart==0.0.6
openai==1.3.7
python-dotenv==1.0.0
pydantic==2.5.0
```

```python
# main.py
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
from services.twitter_scraper import TwitterScraper
from services.image_analyzer import ImageAnalyzer
from services.gpt_service import GPTService

app = FastAPI(title="Twitter Analyzer API", version="1.0.0")

# CORS налаштування
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class AnalyzeRequest(BaseModel):
    twitter_url: str

class AnalyzeResponse(BaseModel):
    comments: list[str]
    status: str
    post_text: str
    image_descriptions: list[str]

@app.post("/api/v1/analyze", response_model=AnalyzeResponse)
async def analyze_twitter_post(request: AnalyzeRequest):
    try:
        # Парсинг Twitter посту
        scraper = TwitterScraper()
        post_data = await scraper.scrape_post(request.twitter_url)
        
        # Аналіз зображень
        analyzer = ImageAnalyzer()
        image_descriptions = await analyzer.analyze_images(post_data.image_urls)
        
        # Генерація коментарів
        gpt_service = GPTService()
        comments = await gpt_service.generate_comments(post_data.text, image_descriptions)
        
        return AnalyzeResponse(
            comments=comments,
            status="success",
            post_text=post_data.text,
            image_descriptions=image_descriptions
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/health")
async def health_check():
    return {"status": "healthy"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 1.2 Twitter Scraper

```python
# services/twitter_scraper.py
import requests
from bs4 import BeautifulSoup
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import asyncio
from typing import List, Optional

class PostData:
    def __init__(self, text: str, image_urls: List[str], timestamp: str):
        self.text = text
        self.image_urls = image_urls
        self.timestamp = timestamp

class TwitterScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def validate_url(self, url: str) -> bool:
        """Валідація Twitter URL"""
        pattern = r'https?://(?:www\.)?twitter\.com/\w+/status/\d+'
        return bool(re.match(pattern, url))
    
    async def scrape_post(self, url: str) -> PostData:
        """Парсинг Twitter посту"""
        if not self.validate_url(url):
            raise ValueError("Invalid Twitter URL")
        
        try:
            # Використання Selenium для рендерингу JavaScript
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            
            driver = webdriver.Chrome(options=chrome_options)
            driver.get(url)
            
            # Очікування завантаження контенту
            await asyncio.sleep(3)
            
            # Отримання HTML
            html = driver.page_source
            driver.quit()
            
            # Парсинг з BeautifulSoup
            soup = BeautifulSoup(html, 'html.parser')
            
            # Вилучення тексту посту
            text = self._extract_text(soup)
            
            # Вилучення зображень
            image_urls = self._extract_images(soup)
            
            # Вилучення timestamp
            timestamp = self._extract_timestamp(soup)
            
            return PostData(text, image_urls, timestamp)
            
        except Exception as e:
            raise Exception(f"Failed to scrape Twitter post: {str(e)}")
    
    def _extract_text(self, soup: BeautifulSoup) -> str:
        """Вилучення тексту посту"""
        # Пошук елементів з текстом посту
        text_elements = soup.find_all('div', {'data-testid': 'tweetText'})
        if text_elements:
            return text_elements[0].get_text(strip=True)
        
        # Альтернативний пошук
        text_elements = soup.find_all('div', class_='css-901oao')
        if text_elements:
            return text_elements[0].get_text(strip=True)
        
        return ""
    
    def _extract_images(self, soup: BeautifulSoup) -> List[str]:
        """Вилучення URL зображень"""
        image_urls = []
        
        # Пошук зображень в твіті
        images = soup.find_all('img', {'alt': True})
        for img in images:
            src = img.get('src')
            if src and 'pbs.twimg.com' in src:
                # Отримання повного URL зображення
                full_url = self._get_full_image_url(src)
                if full_url:
                    image_urls.append(full_url)
        
        return image_urls
    
    def _get_full_image_url(self, src: str) -> Optional[str]:
        """Отримання повного URL зображення"""
        if '?format=' in src:
            return src.split('?')[0] + '?format=jpg&name=large'
        return src
    
    def _extract_timestamp(self, soup: BeautifulSoup) -> str:
        """Вилучення timestamp посту"""
        time_elements = soup.find_all('time')
        if time_elements:
            return time_elements[0].get('datetime', '')
        return ""
```

### 1.3 Image Analyzer

```python
# services/image_analyzer.py
import cv2
import numpy as np
import requests
from PIL import Image
import io
from typing import List, Optional

class ImageAnalyzer:
    def __init__(self):
        self.object_categories = [
            'person', 'car', 'dog', 'cat', 'building', 'tree', 
            'food', 'book', 'phone', 'computer', 'nature', 'city'
        ]
    
    async def analyze_images(self, image_urls: List[str]) -> List[str]:
        """Аналіз списку зображень"""
        descriptions = []
        
        for url in image_urls:
            try:
                description = await self.analyze_single_image(url)
                descriptions.append(description)
            except Exception as e:
                descriptions.append(f"Error analyzing image: {str(e)}")
        
        return descriptions
    
    async def analyze_single_image(self, image_url: str) -> str:
        """Аналіз одного зображення"""
        try:
            # Завантаження зображення
            response = requests.get(image_url, timeout=10)
            response.raise_for_status()
            
            # Конвертація в PIL Image
            image = Image.open(io.BytesIO(response.content))
            
            # Конвертація в OpenCV формат
            cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            
            # Базовий аналіз зображення
            description = self._analyze_image_content(cv_image)
            
            return description
            
        except Exception as e:
            return f"Failed to analyze image: {str(e)}"
    
    def _analyze_image_content(self, image) -> str:
        """Базовий аналіз змісту зображення"""
        # Конвертація в grayscale для аналізу
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # Аналіз яскравості
        brightness = np.mean(gray)
        
        # Аналіз контрасту
        contrast = np.std(gray)
        
        # Виявлення країв
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / edges.size
        
        # Визначення типу зображення
        image_type = self._classify_image(brightness, contrast, edge_density)
        
        # Виявлення основних кольорів
        colors = self._extract_dominant_colors(image)
        
        return f"{image_type} image with {colors} colors, brightness: {brightness:.1f}, contrast: {contrast:.1f}"
    
    def _classify_image(self, brightness: float, contrast: float, edge_density: float) -> str:
        """Класифікація типу зображення"""
        if brightness < 50:
            return "dark"
        elif brightness > 200:
            return "bright"
        elif contrast > 50:
            return "high-contrast"
        elif edge_density > 0.1:
            return "detailed"
        else:
            return "simple"
    
    def _extract_dominant_colors(self, image) -> str:
        """Вилучення домінуючих кольорів"""
        # Зменшення розміру для швидшого аналізу
        small_image = cv2.resize(image, (50, 50))
        
        # Конвертація в RGB
        rgb_image = cv2.cvtColor(small_image, cv2.COLOR_BGR2RGB)
        
        # Перетворення в список пікселів
        pixels = rgb_image.reshape(-1, 3)
        
        # Використання k-means для знаходження домінуючих кольорів
        from sklearn.cluster import KMeans
        
        kmeans = KMeans(n_clusters=3, random_state=42)
        kmeans.fit(pixels)
        
        # Отримання домінуючих кольорів
        colors = kmeans.cluster_centers_
        
        # Конвертація в назви кольорів
        color_names = []
        for color in colors:
            color_name = self._rgb_to_color_name(color)
            color_names.append(color_name)
        
        return ", ".join(color_names)
    
    def _rgb_to_color_name(self, rgb) -> str:
        """Конвертація RGB в назву кольору"""
        r, g, b = rgb
        
        if r > 200 and g > 200 and b > 200:
            return "white"
        elif r < 50 and g < 50 and b < 50:
            return "black"
        elif r > 200 and g < 100 and b < 100:
            return "red"
        elif r < 100 and g > 200 and b < 100:
            return "green"
        elif r < 100 and g < 100 and b > 200:
            return "blue"
        elif r > 200 and g > 200 and b < 100:
            return "yellow"
        else:
            return "mixed"
```

## 2. Етап 2: Інтеграція з ChatGPT та генерація коментарів (2 тижні)

### 2.1 GPT Service

```python
# services/gpt_service.py
import openai
import os
from typing import List
import asyncio

class GPTService:
    def __init__(self):
        self.client = openai.AsyncOpenAI(
            api_key=os.getenv("OPENAI_API_KEY")
        )
        self.model = "gpt-4o-mini"
    
    async def generate_comments(self, post_text: str, image_descriptions: List[str]) -> List[str]:
        """Генерація коментарів до посту"""
        try:
            # Формування промпту
            prompt = self._create_prompt(post_text, image_descriptions)
            
            # Виклик OpenAI API
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": """Ти експерт з аналізу соціальних мереж. 
                        Проаналізуй Twitter-пост та згенеруй 3-5 релевантних коментарів. 
                        Коментарі мають бути різноманітними за стилем та підходом:
                        - Один коментар може бути підтримуючим
                        - Один коментар може бути критичним
                        - Один коментар може бути запитанням
                        - Один коментар може бути гуморним
                        - Один коментар може бути інформативним
                        
                        Відповідай тільки списком коментарів, кожен з нового рядка."""
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=500,
                temperature=0.7
            )
            
            # Парсинг відповіді
            comments_text = response.choices[0].message.content
            comments = self._parse_comments(comments_text)
            
            return comments
            
        except Exception as e:
            raise Exception(f"Failed to generate comments: {str(e)}")
    
    def _create_prompt(self, post_text: str, image_descriptions: List[str]) -> str:
        """Створення промпту для GPT"""
        prompt = f"Текст посту: {post_text}\n\n"
        
        if image_descriptions:
            prompt += "Зображення в пості:\n"
            for i, desc in enumerate(image_descriptions, 1):
                prompt += f"{i}. {desc}\n"
            prompt += "\n"
        
        prompt += "Згенеруй 3-5 різноманітних коментарів до цього посту."
        
        return prompt
    
    def _parse_comments(self, comments_text: str) -> List[str]:
        """Парсинг згенерованих коментарів"""
        # Розділення на окремі коментарі
        lines = comments_text.strip().split('\n')
        
        comments = []
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#'):
                # Видалення номерів та маркерів
                if line[0].isdigit() and line[1] in ['.', ')', '-']:
                    line = line[2:].strip()
                elif line.startswith('- '):
                    line = line[2:].strip()
                
                if line:
                    comments.append(line)
        
        return comments[:5]  # Обмеження до 5 коментарів
```

### 2.2 Оновлений main.py з обробкою помилок

```python
# main.py (оновлена версія)
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, HttpUrl
import logging
from typing import List, Optional
import time

# Налаштування логування
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Twitter Analyzer API", version="1.0.0")

# CORS налаштування
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class AnalyzeRequest(BaseModel):
    twitter_url: HttpUrl

class AnalyzeResponse(BaseModel):
    comments: List[str]
    status: str
    post_text: str
    image_descriptions: List[str]
    processing_time: float
    error: Optional[str] = None

@app.post("/api/v1/analyze", response_model=AnalyzeResponse)
async def analyze_twitter_post(request: AnalyzeRequest, background_tasks: BackgroundTasks):
    start_time = time.time()
    
    try:
        logger.info(f"Starting analysis for URL: {request.twitter_url}")
        
        # Парсинг Twitter посту
        scraper = TwitterScraper()
        post_data = await scraper.scrape_post(str(request.twitter_url))
        
        if not post_data.text:
            raise HTTPException(status_code=400, detail="No text content found in the post")
        
        # Аналіз зображень
        analyzer = ImageAnalyzer()
        image_descriptions = await analyzer.analyze_images(post_data.image_urls)
        
        # Генерація коментарів
        gpt_service = GPTService()
        comments = await gpt_service.generate_comments(post_data.text, image_descriptions)
        
        processing_time = time.time() - start_time
        logger.info(f"Analysis completed in {processing_time:.2f} seconds")
        
        return AnalyzeResponse(
            comments=comments,
            status="success",
            post_text=post_data.text,
            image_descriptions=image_descriptions,
            processing_time=processing_time
        )
        
    except HTTPException:
        raise
    except Exception as e:
        processing_time = time.time() - start_time
        logger.error(f"Analysis failed: {str(e)}")
        
        return AnalyzeResponse(
            comments=[],
            status="error",
            post_text="",
            image_descriptions=[],
            processing_time=processing_time,
            error=str(e)
        )

@app.get("/api/v1/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": time.time(),
        "version": "1.0.0"
    }

@app.get("/api/v1/status")
async def get_status():
    return {
        "api_status": "running",
        "openai_available": True,  # Можна додати реальну перевірку
        "twitter_scraping_available": True
    }
```

## 3. Етап 3: Frontend розробка (1 тиждень)

### 3.1 Створення React додатку

```bash
# Створення React додатку
npx create-react-app twitter-analyzer-frontend
cd twitter-analyzer-frontend

# Встановлення залежностей
npm install axios @mui/material @emotion/react @emotion/styled
npm install @mui/icons-material react-router-dom
```

### 3.2 Основний компонент App.jsx

```jsx
// src/App.jsx
import React, { useState } from 'react';
import {
  Container,
  Typography,
  Box,
  CssBaseline,
  ThemeProvider,
  createTheme
} from '@mui/material';
import UrlInput from './components/UrlInput';
import CommentList from './components/CommentList';
import LoadingSpinner from './components/LoadingSpinner';
import ErrorMessage from './components/ErrorMessage';

const theme = createTheme({
  palette: {
    primary: {
      main: '#1da1f2', // Twitter blue
    },
    secondary: {
      main: '#14171a',
    },
  },
});

function App() {
  const [analysisResult, setAnalysisResult] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);

  const handleAnalysis = async (twitterUrl) => {
    setLoading(true);
    setError(null);
    setAnalysisResult(null);

    try {
      const response = await fetch('http://localhost:8000/api/v1/analyze', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ twitter_url: twitterUrl }),
      });

      const data = await response.json();

      if (!response.ok) {
        throw new Error(data.detail || 'Analysis failed');
      }

      setAnalysisResult(data);
    } catch (err) {
      setError(err.message);
    } finally {
      setLoading(false);
    }
  };

  return (
    <ThemeProvider theme={theme}>
      <CssBaseline />
      <Container maxWidth="md">
        <Box sx={{ my: 4 }}>
          <Typography variant="h3" component="h1" gutterBottom align="center">
            Twitter Post Analyzer
          </Typography>
          <Typography variant="h6" component="h2" gutterBottom align="center" color="text.secondary">
            Аналіз Twitter-постів та генерація коментарів
          </Typography>

          <UrlInput onAnalyze={handleAnalysis} disabled={loading} />

          {loading && <LoadingSpinner />}

          {error && <ErrorMessage message={error} />}

          {analysisResult && (
            <CommentList 
              comments={analysisResult.comments}
              postText={analysisResult.post_text}
              imageDescriptions={analysisResult.image_descriptions}
              processingTime={analysisResult.processing_time}
            />
          )}
        </Box>
      </Container>
    </ThemeProvider>
  );
}

export default App;
```

### 3.3 Компонент UrlInput

```jsx
// src/components/UrlInput.jsx
import React, { useState } from 'react';
import {
  Box,
  TextField,
  Button,
  Paper,
  Typography,
  Alert
} from '@mui/material';
import { Twitter as TwitterIcon } from '@mui/icons-material';

const UrlInput = ({ onAnalyze, disabled }) => {
  const [url, setUrl] = useState('');
  const [urlError, setUrlError] = useState('');

  const validateUrl = (url) => {
    const twitterPattern = /^https?:\/\/(www\.)?twitter\.com\/\w+\/status\/\d+$/;
    return twitterPattern.test(url);
  };

  const handleSubmit = (e) => {
    e.preventDefault();
    
    if (!url.trim()) {
      setUrlError('Будь ласка, введіть URL');
      return;
    }

    if (!validateUrl(url)) {
      setUrlError('Будь ласка, введіть валідний Twitter URL');
      return;
    }

    setUrlError('');
    onAnalyze(url);
  };

  return (
    <Paper elevation={3} sx={{ p: 3, mb: 3 }}>
      <Typography variant="h6" gutterBottom>
        Введіть посилання на Twitter-пост
      </Typography>
      
      <Box component="form" onSubmit={handleSubmit} sx={{ mt: 2 }}>
        <TextField
          fullWidth
          label="Twitter URL"
          value={url}
          onChange={(e) => setUrl(e.target.value)}
          placeholder="https://twitter.com/username/status/123456789"
          error={!!urlError}
          helperText={urlError}
          disabled={disabled}
          sx={{ mb: 2 }}
        />
        
        <Button
          type="submit"
          variant="contained"
          size="large"
          disabled={disabled || !url.trim()}
          startIcon={<TwitterIcon />}
          fullWidth
        >
          {disabled ? 'Аналізуємо...' : 'Аналізувати пост'}
        </Button>
      </Box>

      <Alert severity="info" sx={{ mt: 2 }}>
        Приклад: https://twitter.com/username/status/123456789
      </Alert>
    </Paper>
  );
};

export default UrlInput;
```

### 3.4 Компонент CommentList

```jsx
// src/components/CommentList.jsx
import React from 'react';
import {
  Paper,
  Typography,
  List,
  ListItem,
  ListItemText,
  ListItemIcon,
  Box,
  Chip,
  Divider
} from '@mui/material';
import { Comment as CommentIcon, Image as ImageIcon, AccessTime as TimeIcon } from '@mui/icons-material';

const CommentList = ({ comments, postText, imageDescriptions, processingTime }) => {
  const copyToClipboard = (text) => {
    navigator.clipboard.writeText(text);
  };

  return (
    <Paper elevation={3} sx={{ p: 3 }}>
      <Typography variant="h6" gutterBottom>
        Результати аналізу
      </Typography>

      {/* Інформація про обробку */}
      <Box sx={{ mb: 3, p: 2, bgcolor: 'grey.50', borderRadius: 1 }}>
        <Typography variant="body2" color="text.secondary" sx={{ display: 'flex', alignItems: 'center', mb: 1 }}>
          <TimeIcon sx={{ mr: 1, fontSize: 16 }} />
          Час обробки: {processingTime.toFixed(2)} секунд
        </Typography>
        
        {imageDescriptions.length > 0 && (
          <Typography variant="body2" color="text.secondary" sx={{ display: 'flex', alignItems: 'center' }}>
            <ImageIcon sx={{ mr: 1, fontSize: 16 }} />
            Знайдено зображень: {imageDescriptions.length}
          </Typography>
        )}
      </Box>

      {/* Оригінальний текст посту */}
      {postText && (
        <Box sx={{ mb: 3 }}>
          <Typography variant="subtitle1" gutterBottom>
            Оригінальний текст:
          </Typography>
          <Paper variant="outlined" sx={{ p: 2, bgcolor: 'grey.50' }}>
            <Typography variant="body2">{postText}</Typography>
          </Paper>
        </Box>
      )}

      {/* Згенеровані коментарі */}
      <Typography variant="h6" gutterBottom>
        Згенеровані коментарі ({comments.length}):
      </Typography>

      <List>
        {comments.map((comment, index) => (
          <React.Fragment key={index}>
            <ListItem 
              sx={{ 
                cursor: 'pointer',
                '&:hover': { bgcolor: 'action.hover' }
              }}
              onClick={() => copyToClipboard(comment)}
            >
              <ListItemIcon>
                <CommentIcon color="primary" />
              </ListItemIcon>
              <ListItemText
                primary={comment}
                secondary={`Клікніть для копіювання`}
              />
              <Chip label={`#${index + 1}`} size="small" color="primary" />
            </ListItem>
            {index < comments.length - 1 && <Divider />}
          </React.Fragment>
        ))}
      </List>

      {/* Опис зображень */}
      {imageDescriptions.length > 0 && (
        <Box sx={{ mt: 3 }}>
          <Typography variant="subtitle1" gutterBottom>
            Аналіз зображень:
          </Typography>
          <List dense>
            {imageDescriptions.map((desc, index) => (
              <ListItem key={index}>
                <ListItemIcon>
                  <ImageIcon color="secondary" />
                </ListItemIcon>
                <ListItemText primary={desc} />
              </ListItem>
            ))}
          </List>
        </Box>
      )}
    </Paper>
  );
};

export default CommentList;
```

## 4. Етап 4: Тестування та оптимізація (1 тиждень)

### 4.1 Unit тести для Backend

```python
# tests/test_twitter_scraper.py
import pytest
from services.twitter_scraper import TwitterScraper

class TestTwitterScraper:
    def setup_method(self):
        self.scraper = TwitterScraper()
    
    def test_validate_url_valid(self):
        url = "https://twitter.com/username/status/123456789"
        assert self.scraper.validate_url(url) == True
    
    def test_validate_url_invalid(self):
        urls = [
            "https://facebook.com/post/123",
            "https://twitter.com/username",
            "invalid_url",
            "https://twitter.com/username/status/abc"
        ]
        for url in urls:
            assert self.scraper.validate_url(url) == False
    
    @pytest.mark.asyncio
    async def test_scrape_post_mock(self):
        # Мок тест для парсингу
        # Тут можна використовувати мок-дані
        pass
```

### 4.2 Integration тести

```python
# tests/test_api.py
import pytest
from fastapi.testclient import TestClient
from main import app

client = TestClient(app)

def test_health_check():
    response = client.get("/api/v1/health")
    assert response.status_code == 200
    assert response.json()["status"] == "healthy"

def test_analyze_invalid_url():
    response = client.post("/api/v1/analyze", json={
        "twitter_url": "https://invalid-url.com"
    })
    assert response.status_code == 500

def test_analyze_valid_url():
    # Тест з валідним URL (потребує реального посту)
    response = client.post("/api/v1/analyze", json={
        "twitter_url": "https://twitter.com/test/status/123456789"
    })
    # Очікуємо помилку, оскільки пост не існує
    assert response.status_code in [400, 500]
```

## 5. Етап 5: Розгортання та документація (1 тиждень)

### 5.1 Docker конфігурація

```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

# Встановлення системних залежностей
RUN apt-get update && apt-get install -y \
    wget \
    gnupg \
    unzip \
    && rm -rf /var/lib/apt/lists/*

# Встановлення Chrome для Selenium
RUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \
    && echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google.list \
    && apt-get update \
    && apt-get install -y google-chrome-stable \
    && rm -rf /var/lib/apt/lists/*

# Копіювання залежностей
COPY requirements.txt .

# Встановлення Python залежностей
RUN pip install --no-cache-dir -r requirements.txt

# Копіювання коду
COPY . .

# Відкриття порту
EXPOSE 8000

# Запуск додатку
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 5.2 Docker Compose

```yaml
# docker-compose.yml
version: '3.8'

services:
  backend:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - DATABASE_URL=${DATABASE_URL}
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped

  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    depends_on:
      - backend
    environment:
      - REACT_APP_API_URL=http://localhost:8000/api/v1
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - backend
      - frontend
    restart: unless-stopped
```

### 5.3 Nginx конфігурація

```nginx
# nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream backend {
        server backend:8000;
    }

    upstream frontend {
        server frontend:3000;
    }

    server {
        listen 80;
        server_name localhost;

        location /api/ {
            proxy_pass http://backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
        }

        location / {
            proxy_pass http://frontend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
        }
    }
}
```

## 6. Запуск проекту

### 6.1 Локальний запуск

```bash
# Backend
cd twitter-analyzer-backend
python -m venv venv
source venv/bin/activate  # Linux
pip install -r requirements.txt
python main.py

# Frontend (в новому терміналі)
cd twitter-analyzer-frontend
npm install
npm start
```

### 6.2 Docker запуск

```bash
# З Docker Compose
docker-compose up --build

# Або окремо
docker build -t twitter-analyzer-backend .
docker run -p 8000:8000 twitter-analyzer-backend
```

## 7. Моніторинг та логування

### 7.1 Логування

```python
# utils/logger.py
import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': record.levelname,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno
        }
        return json.dumps(log_entry)

def setup_logger():
    logger = logging.getLogger('twitter_analyzer')
    logger.setLevel(logging.INFO)
    
    handler = logging.StreamHandler()
    handler.setFormatter(JSONFormatter())
    logger.addHandler(handler)
    
    return logger
```

### 7.2 Метрики

```python
# utils/metrics.py
import time
from functools import wraps
from collections import defaultdict
import threading

class MetricsCollector:
    def __init__(self):
        self.request_count = 0
        self.response_times = []
        self.error_count = 0
        self.lock = threading.Lock()
    
    def record_request(self, response_time, success=True):
        with self.lock:
            self.request_count += 1
            self.response_times.append(response_time)
            if not success:
                self.error_count += 1
    
    def get_metrics(self):
        with self.lock:
            avg_response_time = sum(self.response_times) / len(self.response_times) if self.response_times else 0
            return {
                'total_requests': self.request_count,
                'average_response_time': avg_response_time,
                'error_rate': self.error_count / self.request_count if self.request_count > 0 else 0
            }

metrics = MetricsCollector()

def track_metrics(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = await func(*args, **kwargs)
            metrics.record_request(time.time() - start_time, success=True)
            return result
        except Exception as e:
            metrics.record_request(time.time() - start_time, success=False)
            raise
    return wrapper
```

Цей план реалізації надає детальні інструкції для створення повнофункціонального додатку аналізу Twitter-постів з генерацією коментарів за допомогою GPT-4o-mini.
